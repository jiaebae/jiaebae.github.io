<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[jiaebae.github.io]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>jiaebae.github.io</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Tue, 30 Jul 2024 07:53:56 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Tue, 30 Jul 2024 07:53:56 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[week4]]></title><description><![CDATA[ 
 <br><br><br><br>
<br>VGGNet에 비해 더 많은 층을 사용
<br>2015년 이미지넷 대회 우승
<br>VGGNet의 한계

<br>너무 많은 층을 사용하면 성능 떨어짐
<br>Gradient Vanishing Problem


<br>특징

<br>많은 수의 네트워크 층을 사용하여 복잡한 특징을 학습할 수 있음
<br>깊음 네트워크 층의 학습을 더 정확하게 할 수 있음


<br><br>ResNet의 핵심 구성 요소는 Residual Block임.<br>
기존 망과의 차이는 입력값을 출력값에 더해줄 수 있도록 지름길(shortcut)을 하나 만들어 준 것 뿐임.<br>
<img alt="스크린샷 2024-07-23 204444.png" src="https://github.com/jiaebae/jiaebae.github.io/blob/main/docs/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20204444.png" referrerpolicy="no-referrer"><br>기존의 신경망은 입력값 x를 타겟값 y로 매핑하는 함수 H(x)를 얻는 것이 목적이었음. 그러나  ResNet은 F(x) + x를 최소화하는 것을 목적으로 함.<br>
x는 현시점에서 변할 수 없는 값이므로 F(x)를 0에 가깝게 만드는 것이 목적이 됨. F(x)가 0이 되면 출력과 입력이 모두 x로 같아지게 됨. F(x) = H(x) - x이므로 F(x)를 최소로 해주다는 것은 H(x) - x를 최소로 해주는 것과 동일한 의미를 지님. 여기서 H(x)-x를 잔차(residual)라고 함.<br><br>2개 이상의 Convolutional Layer와 skip-connection을 활용해 하나의 블록을 만들고 그 블록을 쌓아서 네트워크를 만듦.<br>
<img alt="스크린샷 2024-07-23 203628.png" src="https://github.com/jiaebae/jiaebae.github.io/blob/main/docs/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20203628.png" referrerpolicy="no-referrer"><br>
위 그림의 Residual Block은 l번째 블록으로 xl​을 입력으로 받고 skip-connection인 h(xl)과 Connvolutional layer F(xl, Wl)를 통과한 결과의 합으로 yl을 출력함. 마지막으로 출력 yl을 활성함수를 통과시키면 다음 블록의 입력 xl+1이 됨.<br>ResNet은 기본적으로 VGG-19의 구조를 뼈대로 함.<br>
뼈대에 Connvolutional layer들을 추가해서 깊게 만든 후에, shortcut들을 추가하는 것이 전부다.<br>
34층의 ResNet과 거기에서 shortcut들을 제외한 버전인 plain 네트워크의 구조는 다음과 같다.<br><img alt="b6yg1bA.jpg" src="https://github.com/jiaebae/jiaebae.github.io/blob/main/docs/lib/media/b6yg1bA.jpg" referrerpolicy="no-referrer"><br>위 그림을 보면 알 수 있듯이 34층의 ResNet은 처음을 제외하고는 균일하게 3 x 3 사이즈의 컨볼루션 필터를 사용함.<br>
특성맵의 사이즈가 반으로 줄어들 때, 특성맵의 깊이를 2배로 높임.]]></description><link>week4.html</link><guid isPermaLink="false">week4.md</guid><pubDate>Tue, 30 Jul 2024 07:53:28 GMT</pubDate><enclosure url="https://github.com/jiaebae/jiaebae.github.io/blob/main/docs/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20204444.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://github.com/jiaebae/jiaebae.github.io/blob/main/docs/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20204444.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>