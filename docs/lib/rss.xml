<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[jiaebae.github.io]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>jiaebae.github.io</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 15 Aug 2024 13:33:00 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 15 Aug 2024 13:32:59 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[week7]]></title><description><![CDATA[ 
 <br><br><br><br><img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-13%20204051.png" referrerpolicy="no-referrer"><br>class ResNet(nn.Module):
	def __init__(self.num_classes=10):
		super(ResNet, self).__init__()
		
		# 기본 블록
		self.b1 = BasicBlock(in_channels=3, out_channels=64)
		self.b2 =  BasicBlock(in_channels=64, out_channels=128)
		self.b3 =  BasicBlock(in_channels=128, out_channels=256)
		
		# 풀링을 최댓값이 아닌 평균값으로
		self.pool = nn.AvgPool2d(kernel_size=2, stride=2)
		
		# 분류기
		self.fc1 = nn.Linear(in_features=4096, out_features=2048)
		self.fc2 = nn.Linear(in_features=4096, out_features=2048)
		self.fc3 = nn.Linear(in_features=512, out_features=num_classes)
		
		self.relu = nn.ReLu()
복사<br>10개의 클래스를 갖도록 num_classes를 10으로 저장함.<br>
super()를 이용하여 모듈의 요소들을 불러옴.<br>
기본 블록을 3개 사용하기 때문에 3개 만들어 줌.<br>
입력으로 채널 수가 3이 들어와서 최종적으로 채널 수 256이 나가는 형태가 됨.<br>
풀링은 평균풀링을 사용함. 이미지의 크기를 반으로 줄이기 위해서 kernel_size=2, stride=2로 함.<br>
stride는 커널의 이동거리임.<br>
분류기는 3개의 MLP층을 가짐.<br>
4096개의 특징을 받아서 최종적으로 10개 클래스에 대한 확률값을 내보냄.<br>
활성화 함수로는 ReLu를 사용함.<br>ResNet의 순전파 정의<br>def forward(self, x):
	# 기본 블록과 풀링층 통과
	x = self.b1(x)
	x = self.pool(x)
	x = self.b2(x)
	x = self.pool(x)
	x = self.b3(x)
	x = self.pool(x)
	
	# 분류기의 입력으로 사용하기 위한 평탄화
	x = torch.flatten(x, start_dim=1)
	
	# 분류기로 예측값 출력
	x = self.fc1(x)
	x = self.relu(x)
	x = self.fc2(x)
	x = self.relu(x)
	x = self.fc3(x)
	
	return x
복사<br>첫번째 블럭 지나고 풀링 해주고, 두번째 블럭 지나고 풀링 해주고, 세번째 블럭 지나고 풀링 해줌.<br>
기본 블럭과 풀링층을 통과 했을 때까지는 2차원 이미지의 형태임.<br>
출력값은 1차원 벡터이기 때문에 평탄화 작업을 해줌.<br>
평탄화 한 값을 분류기에 넣어 최종적인 예측값을 출력함.<br><br><img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-14%20175116.png" referrerpolicy="no-referrer"><br>데이터 전처리 정의<br>import tqdm

from torchvision.datasets.cifar import CIFAR10
from torchvision.transforms import Compose, ToTensor
from torchvision.transforms import RandomHorizontalFlip, RandomCrop
from torchvision.transforms import Normalize
from torchvision.utils.data.datloader import DataLoader

from torch.optim.adam import Adam

transforms = Compose[
	RandomCrop((32,32), padding=4), # 랜덤 크롭핑
	RandomHorizontalFlip(p=0.5), # 랜덤 y축 대칭
	ToTensor(),
	Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))
]
복사<br>Compose를 통해 전처리를 모아줌.<br>
랜덤 크롭핑을 통해 필요한 부분(32x32)만 보고 세로 4개에 픽셀을 0으로 채움.<br>
랜덤하게 y축 대칭을 해주고 이미지를 텐서로 바꿔줌.<br>
Normalize()를 통해 이미지를 정규화해줌.<br>데이터불러오기<br># 데이터셋 정의
training_data = CIFAR10(root="./", train=True, download=True, transform=transforms)
test_data = CIFAR10(root="./", train=False, download=True, transform=transforms)

# 데이터로더 정의
train_loader = DataLoader(training_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)
복사<br>모델 정의하기<br>device = "cuda" if torch.cuda.is_available() else "cpu"

model = ResNet(num_classes=10)
model.to(device)
복사<br>device는 gpu면 cuda를 사용하고 아니면 cpu를 사용함.<br>
모델은 ResNet이고 클래스는 10개를 사용함.<br>
모델을 device로 보냄.<br>학습 루프 정의<br>lr = 1e-4
optim = Adam(model.parameters(), lr=lr)

for epoch in range(30):
	iterator = tqdm.tqdm(train_loader)
	for data, label in iterator:
	
		# 최적화를 위해 기울기를 초기화
		optim.zero_grad()
		
		# 모델의 예측값
		preds = model(data.to(device))
		
		# 손실 계산 및 역전파
		loss = nn.CrossEntropyLoss()(preds, label.to(device))
		loss.backward()
		optim.step()
		iterator.set_description(f"epoch:{epoch+1} loss:{loss.item()}")
torch.save(model.state_dict(), "ResNet.pth")
복사<br>학습율(lr) 1e-4 = 10^-4임.<br>
Adam최적화 사용하여 모델의 파라미터를 최적화함. 확률은 미리 정의 했던 학습율이 적용됨.<br>
for문을 통해 총 30번 학습 반복함.<br>
데이터 로더의 프로그래스 바를 위한 tqdm으로 감싸줌.<br>
데이터와 정답을 받아 온 다음에 기울기 초기화 해줌.<br>
모델 예측값 출력한 다음 손실 계산 해주고 오차 역전파하고 최적화 진행해 준 다음 epoch마다 손실 출력 해줌.<br>
학습이 완료된 가중치를 torch.save()를 이용해서 ResNet.pth라는 이름으로 저장함.<br><br>ResNet 성능 확인해보기<br>model.load_state_dict(torch.load("ResNet.pth"), map_location=device)

num_corr = 0

with torch.no_grad():
	for data, label in test_loader:
	
		output = model(data.to(device))
		preds = output.data.max(1)[1]
		corr = preds.eq(label.to(device).data).sum().item()
		num_corr += corr
		
	print(f"Accuracy:{num_corr/leln(test_data)}")
복사<br>"ResNet.pth"를 torch.load를 이용하여 device라는 위치에 불러옴.<br>
전체 맞춘 갯수(num_corr) 0으로 초기화.<br>
기울기 사용하지 않으니 no_grad로 선언.<br>
test_loader로 data와 label 받아온 다음 모델 예측값 내보내 주고, output값의 인덱스만 받아오고 인덱스가 정답과 일치하는지 확인하고 일치한다면 num_corr값 갱신해주고 최종적으로 분류 정확도를 출력해줌.<br><br>Quantization은 실수형 변수(floating-point type)를 정수형 변수(integer or fixed point)로 변환하는 과정을 뜻함.<br>
<br>Quantization은 weight나 activation function의 값이 어느 정도의 범위 안에 있다는 것을 가정하여 이루어지는 모델 경량화 방법임<br>
- &gt; floating point로 학습한 모델의 weight 값이 -10 ~ 30 의 범위에 있다는 경우의 예시에 대해, 최소값인 -10을 uint8의 0에 대응시키고 30을 uint8의 최대값인 255에 대응시켜서 사용한다면 32bit 자료형이 8bit 자료형으로 줄어들기 때문에 전체 메모리 사용량 및 수행 속도가 감소하는 효과를 얻을 수 있음<br>

<br>이와 같이 Quantization을 통하여 효과적인 모델 최적화를 할 수 있는데, float 타입을 int형으로 줄이면서 용량을 줄일 수 있고 bit 수를 줄임으로써 계산 복잡도도 줄일 수 있음<br>

<br>또한 정수형이 하드웨어가 연산하기에 더 간편하다는 장점이 있음

[정리] Listed advantages:

① 모델의 사이즈 축소
② 모델의 연산량 감소
③ 효율적인 하드웨어 사용



]]></description><link>week7.html</link><guid isPermaLink="false">week7.md</guid><pubDate>Thu, 15 Aug 2024 13:32:41 GMT</pubDate><enclosure url="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-13%20204051.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-13%20204051.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[week6]]></title><description><![CDATA[ 
 <br><br><br><br><img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-07%20144641.png" referrerpolicy="no-referrer"><br>ResNet 기본블록<br>import torch
import torch.nn as nn

class BasicBlock(nn.Module):
def __init__(self, in_channels, out_channels, kernel_size=3):
	super(BasicBlock, self).__init__()
	
	#합성곱층 정의
	self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size=kerner_size, padding=1)
	self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size=kerner_size, padding=1)
	self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1)
	
	#배치 정규화층 정의
	self.bn1 = nn.BatchNorm2d(num_features=out_channels)
	self.bn2 = nn.BatchNorm2d(num_features=out_channels)
	
	self.relu = nn.ReLU()
복사<br>import torch
import torch.nn as nn
복사<br>파이토치와 신경망 불러와 줌.<br>class BasicBlock(nn.Module):
def __init__(self, in_channels, out_channels, kernel_size=3):
	super(BasicBlock, self).__init__()
복사<br>BasicBlock이라는 이름으로 클래스 만들어 줌.<br>
초기화 함수를 정의 해 줌. 'in_channels'은 입력 채널 수, 'out_channels'는 출력 채널 수, 'kernel_size'는 합성곱이 갖는 커널의 크기임.<br>
super()를 이용해 nn.Module을 가져옴.<br>#합성곱층 정의
	self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size=kerner_size, padding=1)
	self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size=kerner_size, padding=1)
	self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1)
복사<br>2개의 합성곱층과 다운샘플층을 정의해 줌.<br>#배치 정규화층 정의
	self.bn1 = nn.BatchNorm2d(num_features=out_channels)
	self.bn2 = nn.BatchNorm2d(num_features=out_channels)
	self.relu = nn.ReLU()
복사<br>합성곱을 한 번 거칠 때마다 배치 정규화도 필요하기 때문에 배치 정규화 2개 정의해 줌.<br>self.relu = nn.ReLU()
복사<br>활성화 함수 정의]]></description><link>week6.html</link><guid isPermaLink="false">week6.md</guid><pubDate>Wed, 07 Aug 2024 06:13:07 GMT</pubDate><enclosure url="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-07%20144641.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-08-07%20144641.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[week5]]></title><description><![CDATA[ 
 <br><br><br><br>50층 이상의 깊은 모델에서는 Inception에서와 마찬가지로, 연산상의 이점을 위해 "bottleneck" layer(1x1 convolution)을 이용함.<br><img alt="다운로드" src="https://jiaebae.github.io/lib/media/%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C.png" referrerpolicy="no-referrer"><br>기존의 Residual Block은 한 블록에 Convolution Layer(3X3) 2개가 있는 구조였음. Bottleneck 구조는 오른쪽 그림의 구조로 바꾸었는데 층이 하나 더 생겼지만 Convolution Layer(1X1) 2개를 사용하기 때문에 파라미터 수가 감소하여 연산량이 줄어들었음. 또한 Layer가 많아짐에 따라 Activation Function이 증가하여 더 많은 non-linearity가 들어감. 즉 Input을 기존보다 다양하게 가공할 수 있게 되었음.<br>결론적으로 ResNet은 Skip Connection을 이용한 Shortcut과 Bottleneck 구조를 이용하여 더 깊게 층을 쌓을 수 있었음.<br><br>우리는 쉽게 Optimal depth를 알 수가 없음.<br>
20층이 Optimal인지, 30층이 Optimal인지, 100층이 optimal인지 아무도 모름. 하지만 * degradation problem은 야속하게도 우리는 알 수 없는 optimal depth를 넘어가면 바로 일어남.<br>ResNet은 엄청나게 깊은 네트워크를 만들어주고, Optimal depth에서의 값을 바로 Output으로 보내버릴 수 있는데 가능한 이유는 Skip connection 때문임.<br>
ResNet은 Skip connection이 존재하기 때문에 Main path에서 Optimal depth 이후의 Weight와 Bias가 전부 0에 수렴하도록 학습된다면 Optimal depth에서의 Output이 바로 Classification으로 넘어갈 수 있음. 즉, Optimal depth이후의 block은 모두 빈깡통과 같음.<br>예를 들어 27층이 Optimal depth인데 ResNet 50에서 학습을 한다면, 28층부터 Classification 전까지의 weight와 bias를 전부 0으로 만들어버림. 그러면 27층에서의 output이 바로 Classification에서 이용되고, 이는 Optimal depth의 네트워크를 그대로 사용하는 것과 같다고 볼 수 있음.<br>
<br>

<br>Degradation Problem: 딥러닝 모델의 레이어가 깊어졌을 때 모델이 수렴했음에도 불구하고 오히려 레이어 개수가 적을 때보다 모델의 trainig/test error가 더 커지는 현상이 발생. 이것은 오버피팅 때문이 아니라 네트워크 구조상 레이어를 깊이 쌓았을 때 최적화가 잘 안되기 때문에 발생하는 문제임.


<br><img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-31%20164013.png" referrerpolicy="no-referrer"><br>
layer가 깊어질수록 vanishing gradient 문제는 심화되고 모든 값은 0으로 수렴하여 학습이 진행되지 않지만 입력값은 0으로 수렴되지 않고 남게 됨. 즉, 이미지의 우측하단에 g(a<a data-footref="[inline0" href="about:blank#fn-1-76d8cb535bee22a0" class="footnote-link" target="_self" rel="noopener">[1]</a>)=a[l]로 남아있는 것처럼(여기서 g는 relu는 activation function이며 relu임.) 0으로 수렴되는 상황에서도 최소한 입력값 a[l]만큼의 성능을 유지할 수 있는 것임.<br>
residual learning은 통로를 두 개를 만들어 pain network에서 vanishing gradient가 발생하면 short connection을 통하여 상쇄하는 역할이라 할 수 있음<br>
<br>
<br>l<a href="about:blank#fnref-1-76d8cb535bee22a0" class="footnote-backref footnote-link" target="_self" rel="noopener">↩︎</a>
]]></description><link>week5.html</link><guid isPermaLink="false">week5.md</guid><pubDate>Wed, 31 Jul 2024 10:53:56 GMT</pubDate><enclosure url="https://jiaebae.github.io/lib/media/%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://jiaebae.github.io/lib/media/%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[week4]]></title><description><![CDATA[ 
 <br><br><br><br>
<br>VGGNet에 비해 더 많은 층을 사용
<br>2015년 이미지넷 대회 우승
<br>VGGNet의 한계

<br>너무 많은 층을 사용하면 성능 떨어짐
<br>Gradient Vanishing Problem


<br>특징

<br>많은 수의 네트워크 층을 사용하여 복잡한 특징을 학습할 수 있음
<br>깊음 네트워크 층의 학습을 더 정확하게 할 수 있음


<br><br>ResNet의 핵심 구성 요소는 Residual Block임.<br>
기존 망과의 차이는 입력값을 출력값에 더해줄 수 있도록 지름길(shortcut)을 하나 만들어 준 것 뿐임.<br>
<img alt="스크린샷 2024-07-23 204444.png" src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20204444.png" referrerpolicy="no-referrer"><br>기존의 신경망은 입력값 x를 타겟값 y로 매핑하는 함수 H(x)를 얻는 것이 목적이었음. 그러나  ResNet은 F(x) + x를 최소화하는 것을 목적으로 함.<br>
x는 현시점에서 변할 수 없는 값이므로 F(x)를 0에 가깝게 만드는 것이 목적이 됨. F(x)가 0이 되면 출력과 입력이 모두 x로 같아지게 됨. F(x) = H(x) - x이므로 F(x)를 최소로 해주다는 것은 H(x) - x를 최소로 해주는 것과 동일한 의미를 지님. 여기서 H(x)-x를 잔차(residual)라고 함.<br><br>2개 이상의 Convolutional Layer와 skip-connection을 활용해 하나의 블록을 만들고 그 블록을 쌓아서 네트워크를 만듦.<br>
<img alt="스크린샷 2024-07-23 203628.png" src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20203628.png" referrerpolicy="no-referrer"><br>
위 그림의 Residual Block은 l번째 블록으로 xl​을 입력으로 받고 skip-connection인 h(xl)과 Connvolutional layer F(xl, Wl)를 통과한 결과의 합으로 yl을 출력함. 마지막으로 출력 yl을 활성함수를 통과시키면 다음 블록의 입력 xl+1이 됨.<br>ResNet은 기본적으로 VGG-19의 구조를 뼈대로 함.<br>
뼈대에 Connvolutional layer들을 추가해서 깊게 만든 후에, shortcut들을 추가하는 것이 전부다.<br>
34층의 ResNet과 거기에서 shortcut들을 제외한 버전인 plain 네트워크의 구조는 다음과 같다.<br><img alt="b6yg1bA.jpg" src="https://jiaebae.github.io/lib/media/b6yg1bA.jpg" referrerpolicy="no-referrer"><br>위 그림을 보면 알 수 있듯이 34층의 ResNet은 처음을 제외하고는 균일하게 3 x 3 사이즈의 컨볼루션 필터를 사용함.<br>
특성맵의 사이즈가 반으로 줄어들 때, 특성맵의 깊이를 2배로 높임.]]></description><link>week4.html</link><guid isPermaLink="false">week4.md</guid><pubDate>Tue, 30 Jul 2024 12:01:25 GMT</pubDate><enclosure url="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20204444.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://jiaebae.github.io/lib/media/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-07-23%20204444.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[index]]></title><description><![CDATA[ 
 <br><br>모각코를 하며 방학을 알차게 보내겠습니다:)]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Tue, 09 Jul 2024 14:10:40 GMT</pubDate></item><item><title><![CDATA[Obsidian template for using github.io]]></title><description><![CDATA[ 
 <br>]]></description><link>readme.html</link><guid isPermaLink="false">README.md</guid><pubDate>Wed, 26 Jun 2024 12:40:02 GMT</pubDate></item><item><title><![CDATA[test]]></title><description><![CDATA[ 
 <br>안녕하세요!<br>
테스트 페이지입니다.]]></description><link>test.html</link><guid isPermaLink="false">test.md</guid><pubDate>Wed, 26 Jun 2024 14:11:00 GMT</pubDate></item><item><title><![CDATA[week1]]></title><description><![CDATA[<a class="tag" href="?query=tag:파이토치" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#파이토치</a> <a class="tag" href="?query=tag:Pytorch" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Pytorch</a> <a class="tag" href="?query=tag:합성곱신경망" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#합성곱신경망</a> <a class="tag" href="?query=tag:CNN" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#CNN</a> 
 <br><br><br><br>
<br>Python 기반의 과학 연산 패키지
<br>NumPy를 대체하면서 GPU를 이용한 연산이 필요한 경우 사용
<br>최대한 유연성과 속도를 제공하는 딥러닝 연구 플랫폼이 필요한 경우 사용
<br><br>
<br>텐서플로보다 간결해서 쉽게 사용할 수 있음.
<br>학습 및 추론 속도가 빠르고 다루기 쉬움.
<br>Define-by-Run 프레임워크.<br>
-Define by Run: 동적 계산 그래프를 생성하는 방법.<br>
딥러닝 프레임워크가 순방향 패스(forward pass)를 실행하는 동안 계산 그래프를 생성하게 되는데, 이는 각 반복마다 그래프를 동적으로 변경할 수 있게 함. 이는 정적 계산 그래프(Define and Run)에 비해 더 직관적이며 유연성이 뛰어남.<br>
ex) 재귀 신경망(RNN)
<br>많은 논문들이 파이토치로 구현.
<br>참고:<a rel="noopener" class="external-link" href="https://wikidocs.net/145648" target="_blank">https://wikidocs.net/145648</a><br><br>
<br>

메인 네임스페이스. 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조를 가짐.

<br>

자동 미분을 위한 함수들이 포함되어져 있음. 자동 미분의 on/off를 제어하는 콘텍스트 매니저(enable_grad/no_grad)나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 'Function' 등이 포함되어져 있음.

<br>

신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있음. 예를 들어 RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있음.

<br>

확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있음.

<br>

SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어져 있음.

<br>

ONNX(Open Neural Network Exchange)의 포맷으로 모델을 익스포트(export)할 때 사용. ONNX는 서로 다른 딥 러닝 프레임워크 간에 모델을 공유할 때 사용하는 포맷.

<br>참고:<a rel="noopener" class="external-link" href="https://wikidocs.net/57168" target="_blank">https://wikidocs.net/57168</a><br><br>
<br>딥러닝의 한 종류.
<br>일반적인 신경망 구조에 합성곱 계층(Convolutional Layer)과 풀링 계층(Pooling Layer)을 추가함으로써 공간적인 구조 정보를 유지함.
<br>고차원의 복잡한 데이터를 더욱 간결하게 표현할 수 있게 함.
<br>영상 및 시계열 데이터에서 주요 특징을 찾아내고 학습하기 위한 최적의 아키텍처를 제공함.
<br>참고:<a rel="noopener" class="external-link" href="https://wikidocs.net/120327" target="_blank">https://wikidocs.net/120327</a><br><br>
<br>합성곱 계층(Convolutional Layer): 입력 영상을 일련의 컨벌루션 필터에 통과시킴. 각 필터는 영상에서 특정 특징을 활성화함.
<br>ReLU(Rectified Linear Unit)계층: 음수 값은 0에 매핑, 양수 값은 그대로 두어 더 빠르고 효과적인 훈련이 이루어지도록 함. 이 때 활성화된 특징만 다음 계층으로 전달되므로 이를 활성화라고도 함.
<br>풀링 계층(Pooling Layer): 비성형 다운샘플링을 수행하여 신경망이 학습해야 하는 파라미터의 개수를 줄임으로써 출력을 단순화함.<br>
-위와 같은 연산이 수십 또는 수백 개의 계층에 대해 반복되며, 각 계층은 서로 다른 특징을 식별하도록 학습함. 그 후 분류 계층으로 넘어감.
<br>분류 계층<br>
-마지막에서 두 번째 계층은 k 차원의 벡터를 출력하는 완전 연결 계층이며 분류되는 영상의 각 클래스에 대한 확률을 포함함.<br>
-마지막 계층은 분류 계층을 사용하여 최종 분류 출력을 제공함.
<br><br>
<br>의료 영상: cnn은 수천 건의 병리학 보고서를 검토하여 영상에서 암 세포의 유무를 시각적으로 검출할 수 있음.
<br>오디오 처리: 마이크가 있는 모든 기기에서 키워드 검출을 사용하여 특정 단어나 문구("Hey Siri!")가 발화 되었을 때 이를 검출할 수 있음.
<br>객체 검출: 자율 주행에서 표지판이나 다른 객체의 존재 여부를 정확하게 검출하고 출력을 바탕으로 결정을 내리는 데 사용.
<br>합성 데이터 생성: GAN을 사용하여 얼굴 인식 및 자율 주행을 비롯한 딥 러닝 응용 분야에서 사용할 새로운 영상을 생성할 수 있음.
<br>참고:<a rel="noopener" class="external-link" href="https://kr.mathworks.com/discovery/convolutional-neural-network.html" target="_blank">https://kr.mathworks.com/discovery/convolutional-neural-network.html</a><br><a href=".?query=tag:파이토치" class="tag" target="_blank" rel="noopener">#파이토치</a> <a href=".?query=tag:Pytorch" class="tag" target="_blank" rel="noopener">#Pytorch</a> <a href=".?query=tag:합성곱신경망" class="tag" target="_blank" rel="noopener">#합성곱신경망</a> <a href=".?query=tag:CNN" class="tag" target="_blank" rel="noopener">#CNN</a>]]></description><link>week1.html</link><guid isPermaLink="false">week1.md</guid><pubDate>Sun, 07 Jul 2024 05:54:40 GMT</pubDate></item><item><title><![CDATA[week2]]></title><description><![CDATA[ 
 <br><br><br><br>
<br>OpenAI에서 개발한 자동 음성 인식 모델(Automatic Speech Recognition,ARS).
<br>웹에서 수집한 680,000시간 분량의 다국어 및 멀티태스크 감독 데이터를 기반으로 훈련된 자동 음성 인식(ARS) 시스템.
<br>whisper의 구조는 encoder-decoder transformer로 구현된 간단한 End to End 방식.
<br>zero-shot 성능이 좋음.
<br><br>whisper를 통해 음성을 텍스트로 변환하는 함수<br>def get_whisper():
    model_size = "medium"  #@param ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']
    compute_type = "int8"  #@param ['float16', 'int8']

    return WhisperModel(model_size, device=DEVICE, cpu_threads=12, compute_type=compute_type).transcribe
복사<br>코드 하나씩 뜯어보기<br>
<br>모델 크기 설정('model_size'):
<br>model_size = "medium"
복사<br>'model_size' 변수는 사용할 Whisper 모델의 크기를 지정함.
Whisper 모델은 여러 크기로 제공되며, 작은 크기일수록 더 빠르지만 정확도가 낮을 수 있고,
큰 크기일수록 더 느리지만 더 높은 정확도를 제공함.
복사<br>
<br>계산 유형 설정('compute_type'):
<br>compute_type = "int8"
복사<br>'compute_type' 변수는 모델이 사용하는 숫자 형식을 지정함.
float16 과 int8 중에서 선택할 수 있으며, int8은 더 낮은 정밀도를 가지지만
메모리 사용량과 계산량이 적어 성능 향상에 도움이 될 수 있음.
복사<br>
<br>Whisper 모델 초기화:
<br>WhisperModel(model_size, device=DEVICE, cpu_threads=12, compute_type=compute_type)
복사<br>'WhisperModel' 객체를 초기화함. 이 객체는 다음의 인자를 받음.
- 'model_size': 모델 크기. 이 코드에서는 medium.
- 'device': 모델이 실행될 장치 (예; CPU or GPU). 이 코드에서 DEVICE는 cuda로 정의 되어있음(GPU).
- 'cpu_threads': 모델이 CPU에서 실행될 경우 사용할 thread 수. 이 코드에서는 12개.
- 'compute_type': 계산 유형. 이 코드에서는 int8.
복사<br>
<br>음성 인식 기능 반환:
<br>return WhisperModel(...).transcribe
복사<br>초기화된 'WhisperModel' 객체의 'transcribe' 메서드를 반환함.
transcirbe 메서드는 음성파일을 받아서 텍스트로 변환하는 기능을 수행함.
복사<br>따라서 이 코드는 음성을 텍스트로 변환하는 Whisper 모델을 설정하고, 해당 모델의 음성 인식 기능을 반환하는 함수이다.]]></description><link>week2.html</link><guid isPermaLink="false">week2.md</guid><pubDate>Tue, 09 Jul 2024 14:14:50 GMT</pubDate></item><item><title><![CDATA[week3]]></title><description><![CDATA[ 
 <br><br><br><br>
<br>딥러닝에서 널리 사용되는 인공신경망 구조
<br>2015년 Microsoft의 연구팀이 개발
<br>Residual Connection을 사용하여 심층 신경망에서 발생하는 문제를 해결
<br>사용되는 곳: 이미지 분류, 객체 검출 등.<br>
-일반적으로 신경망이 깊어질수록 더 많은 특징을 학습할 수 있지만, 너무 깊어지면 학습이 어려워지고 성능이 떨어질 수 있음. 이를 "기울기 소실(vanishing gradient)" 문제라고 함. ResNet은 이러한 문제를 해결하기 위해 Residual Connection을 도입함.
<br><br>
<br>입력값을 출력값에 더하는 구조
<br>신경망의 층(layer)을 거친 결과에 원래 입력을 더해줌
<br>y = F(x)+x<br>
-x: 입력값<br>
-F(x): 여러 신경망 층을 거친 출력값<br>
-y: 최종 출력값<br>
이 구조 덕분에 신경망이 깊어지더라도 학습이 잘 진행됨.<br>
입력값을 그대로 출력에 더하기 때문에, 필요한 경우 모델이 단순히 항등 함수를 학습할 수도 있음.<br>
이는 깊은 네트워크에서 중요한 역할을 함.
<br><br>
<br>Residual Connection을 포함한 더 큰 구조
<br>ResNet의 기본 구성 단위
<br>형태

<br>입력 x
<br>두 개의 신경망 층
<br>출력값 F(x)
<br>Residual Connection을 통해 최종 출력값 y = F(x)+x<br>
이 블록들이 여러 개 모여 하나의 ResNet이 됨.


<br><br>
<br>깊은 네트워크 학습 가능: Residual Connection 덕분에 깊은 네트워크도 효과적으로 학습할 수 있음.
<br>성능 향상: 이미지 분류와 같은 다양한 작업에서 뛰어난 성능을 보임.
<br>효율적 구현: 비교적 간단한 구조로도 높은 성능을 발휘함.
<br><br>def get_resnet152():
    model_id = "Wespeaker/wespeaker-voxceleb-resnet152-LM"
    model_name = model_id.replace("Wespeaker/wespeaker-", "").replace("-", "_")

    root_dir = hf_hub_download(model_id, filename=model_name+".onnx").replace(model_name+".onnx", "")

    import os
    if not os.path.isfile(root_dir+"avg_model.pt"):
        os.rename(hf_hub_download(model_id, filename=model_name+".pt"), root_dir+"avg_model.pt")
    if not os.path.isfile(root_dir+"config.yaml"):
        os.rename(hf_hub_download(model_id, filename=model_name+".yaml"), root_dir+"config.yaml")

    resnet = wespeaker.load_model_local(root_dir)

    #print("Compile model for the NPU")
    #resnet.model = intel_npu_acceleration_library.compile(resnet.model)

    def resnet152(ado, sample_rate=None):
        if isinstance(ado, str):
            return resnet.recognize(ado)
        else:
            return recognize(resnet, ado, sample_rate)

    resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)

    return resnet152
복사<br>분석<br>def get_resnet152():
    model_id = "Wespeaker/wespeaker-voxceleb-resnet152-LM"
    model_name = model_id.replace("Wespeaker/wespeaker-", "").replace("-", "_")
복사<br>get_resnet152라는 이름의 함수 정의<br>
'model_id'에 Wespeaker/wespeaker-voxceleb-resnet152-LM을 저장<br>
'model_name'은'model_id'에서 "Wespeaker/wespeaker-"를 제거하고, 하이픈을 언더스코어로 대체하여 생성된 모델 이름. 따라서 voxceleb_resnet152_LM이 모델 이름임.<br>root_dir = hf_hub_download(model_id, filename=model_name+".onnx").replace(model_name+".onnx", "")Your Code
복사<br>'hf_hub_download' 함수는 'model_id'와 파일 이름을 사용하여 모델과 파일을 다운로드함.<br>
다운로드한 파일을 root_dir변수에 저장함.<br>import os
    if not os.path.isfile(root_dir+"avg_model.pt"):
        os.rename(hf_hub_download(model_id, filename=model_name+".pt"), root_dir+"avg_model.pt")
    if not os.path.isfile(root_dir+"config.yaml"):
        os.rename(hf_hub_download(model_id, filename=model_name+".yaml"), root_dir+"config.yaml")
복사<br>os모듈을 임포트함.<br>
avg_model.pt 파일이 root_dir에 없는 경우, '.pt' 파일을 다운로드하여 'avg_model.pt'로 이름을 변경함.<br>
config.yaml파일이 root_dir에 없는 경우, '.yaml'파일을 다운로드하여 'config.yaml'로 이름을 변경함.<br>resnet = wespeaker.load_model_local(root_dir)
복사<br>wespeaker 모듈의 load_madel_local함수를 사용하여 root_dir에서 모델을 로드함.<br>	#print("Compile model for the NPU")
	#resnet.model = intel_npu_acceleration_library.compile(resnet.model)
복사<br>주석 처리되어 있으며, 주석을 해제하면 모델을 NPU(Neural Processing Unit)를 위해 컴파일함.<br>
NPU는 딥러닝 알고리즘 연잔에 최적화된 프로세서임.<br>def resnet152(ado, sample_rate=None):
        if isinstance(ado, str):
            return resnet.recognize(ado)
        else:
            return recognize(resnet, ado, sample_rate)
복사<br>resnet152라는 이름의 내부 함수를 정의함.<br>
입력 ado가 문자열인 경우, resnet 객체의 recognize함수를 호출함.<br>
그렇지 않은 경우, recognize 함수를 호출함.<br>
<br>recognize함수:음성데이터에서 임베딩 벡터를 추출하고, 모델의 임베딩 테이블과 비교하여 가장 유사한 항목을 찾음.
<br>def recognize(model, pcm, sample_rate):
    q = extract_embedding(model, pcm, sample_rate)
    best_score = 0.0
    best_name = ''
    for name, e in model.table.items():
        score = model.cosine_similarity(q, e)
        if best_score &lt; score:
            best_score = score
            best_name = name
        del score
        gc.collect()
    return {'name': best_name, 'confidence': best_score}
복사<br>recognize라는 이름의 함수를 정의 매개변수로 model(음성 인식 모델 객체),pcm(음성 데이터),sample_rate(샘플링 레이트)를 받음<br>
extract_embedding함수는 pcm과 sample_rate를 사용하여 주어진 모델에서 임베딩 벡터를 추출하여 q에 저장함.<br>
best_score는 현재까지 발견된 최고의 유사도 점수를 저장함.<br>
best_name은 현재까지 발견된 최고의 유사도를 가지는 항목의 이름을 저장함.<br>
model.table은 모델이 가지고 있는 임베딩 테이블임. 이 테이블은 name과 e의 쌍으로 이루어져 있음.<br>
e와 q간의 코사인 유사도 점수를 계산함.<br>
계산된 점수가 현재 best_score보다 높으면, best_score와 best_name을 업데이트함.<br>
del score와 gc.collect()는 메모리 관리를 위해 사용됨. 점수를 삭제하고 Garbage Collector를 호출하여 메모리를 해제함.<br>
최종적으로 가장 높은 유사도를 가지는 best_name과 best_score를 딕셔너리 형태로 반환.<br>resnet152.__dict__['register'] = lambda *args, **kwargs: resnet.register(*args, **kwargs)
복사<br>resnet152함수 객체에 register메서드를 추가함. 이는 lambda함수로 resnet객체의 register메서드를 호출함. 함수의 기능은 새로운 데이터를 등록을 함.<br>return resnet152
복사<br>최종적으로 resnet152함수를 반환함.]]></description><link>week3.html</link><guid isPermaLink="false">week3.md</guid><pubDate>Wed, 17 Jul 2024 05:11:21 GMT</pubDate></item></channel></rss>